{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,f1_score,\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Energi_Viborg_Dandas_data.csv\")\n",
    "\n",
    "#drop columns not needed after asking the company about the meaning of these features\n",
    "\n",
    "columns_to_be_removed = ['ID', 'mslink', 'XKoordinat','YKoordinat','LedningID','Dobbeltled','EjerKompon','SystemKode','KategoriAf','DatoUdf']\n",
    "data=data.drop(columns_to_be_removed,axis='columns')\n",
    "\n",
    "# in the column DatoSaneri is the date of repairing and if there is no date it means it is not repaired\n",
    "\n",
    "data['DatoSaneri'].fillna(0, inplace=True)\n",
    "\n",
    "# take only the pipes that are broken(by TV insection) now and the repaired ones\n",
    "\n",
    "data_with_TVObsAndSaneri = data[data['TVObsKode'].isin([1]) | data['DatoSaneri'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get around 2077 rows with not broken pipes\n",
    "\n",
    "data_not_broken = data[~data['TVObsKode'].isin([0]) | data['DatoSaneri'] == 0]\n",
    "data_not_broken = data_not_broken.sample(n=4154) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fra_kote</th>\n",
       "      <th>til_kote</th>\n",
       "      <th>Laengde</th>\n",
       "      <th>Fald</th>\n",
       "      <th>DiameterIn</th>\n",
       "      <th>MaterialeK</th>\n",
       "      <th>anlag_aar</th>\n",
       "      <th>TransportK</th>\n",
       "      <th>Funktionsk</th>\n",
       "      <th>TVObsKode</th>\n",
       "      <th>DatoOprett</th>\n",
       "      <th>DatoOpdate</th>\n",
       "      <th>DatoSaneri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>34.72</td>\n",
       "      <td>33.48</td>\n",
       "      <td>64.88</td>\n",
       "      <td>19.112207</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>39.46</td>\n",
       "      <td>39.16</td>\n",
       "      <td>91.75</td>\n",
       "      <td>3.269755</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>39.71</td>\n",
       "      <td>39.48</td>\n",
       "      <td>87.69</td>\n",
       "      <td>2.622876</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>40.55</td>\n",
       "      <td>40.08</td>\n",
       "      <td>52.11</td>\n",
       "      <td>9.019382</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>40.38</td>\n",
       "      <td>40.55</td>\n",
       "      <td>68.39</td>\n",
       "      <td>-2.485744</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>45.17</td>\n",
       "      <td>45.01</td>\n",
       "      <td>45.10</td>\n",
       "      <td>3.547672</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>40.86</td>\n",
       "      <td>40.74</td>\n",
       "      <td>38.80</td>\n",
       "      <td>3.092784</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16430</th>\n",
       "      <td>19.73</td>\n",
       "      <td>19.47</td>\n",
       "      <td>9.22</td>\n",
       "      <td>28.199566</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>52.16</td>\n",
       "      <td>51.50</td>\n",
       "      <td>78.62</td>\n",
       "      <td>8.394810</td>\n",
       "      <td>160.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22736</th>\n",
       "      <td>40.13</td>\n",
       "      <td>39.52</td>\n",
       "      <td>96.07</td>\n",
       "      <td>6.349537</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6231 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fra_kote  til_kote  Laengde       Fald  DiameterIn  MaterialeK  \\\n",
       "36        34.72     33.48    64.88  19.112207       300.0         1.0   \n",
       "42        39.46     39.16    91.75   3.269755       400.0         1.0   \n",
       "43        39.71     39.48    87.69   2.622876       300.0         1.0   \n",
       "64        40.55     40.08    52.11   9.019382       250.0         1.0   \n",
       "65        40.38     40.55    68.39  -2.485744       250.0         1.0   \n",
       "...         ...       ...      ...        ...         ...         ...   \n",
       "17292     45.17     45.01    45.10   3.547672       300.0         1.0   \n",
       "1746      40.86     40.74    38.80   3.092784       700.0         1.0   \n",
       "16430     19.73     19.47     9.22  28.199566       200.0         1.0   \n",
       "237       52.16     51.50    78.62   8.394810       160.0         4.0   \n",
       "22736     40.13     39.52    96.07   6.349537       600.0         1.0   \n",
       "\n",
       "       anlag_aar  TransportK  Funktionsk  TVObsKode  DatoOprett  DatoOpdate  \\\n",
       "36        1939.0           1           0        0.0        2010        2014   \n",
       "42        1939.0           1           0        1.0        2010        2014   \n",
       "43        1939.0           1           0        1.0        2010        2014   \n",
       "64        1945.0           1           0        1.0        2010        2014   \n",
       "65        1945.0           1           0        1.0        2010        2014   \n",
       "...          ...         ...         ...        ...         ...         ...   \n",
       "17292     1954.0           1           0        0.0        2012        2014   \n",
       "1746      1962.0           1           0        0.0        2010        2014   \n",
       "16430     1964.0           1           0        0.0        2012        2014   \n",
       "237       1975.0           1           0        0.0        2010        2014   \n",
       "22736     1965.0           1           0        0.0        2017        2017   \n",
       "\n",
       "       DatoSaneri  \n",
       "36         1997.0  \n",
       "42            0.0  \n",
       "43            0.0  \n",
       "64            0.0  \n",
       "65            0.0  \n",
       "...           ...  \n",
       "17292         0.0  \n",
       "1746          0.0  \n",
       "16430         0.0  \n",
       "237           0.0  \n",
       "22736         0.0  \n",
       "\n",
       "[6231 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [data_with_TVObsAndSaneri, data_not_broken]\n",
    "  \n",
    "data_final = pd.concat(frames)\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 6231\n",
      "Number of rows after removing NaNs: 6231\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(data.shape[0]))\n",
    "data = data.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data copied\n",
    "datacopy = data\n",
    "\n",
    "\n",
    "# add  age column\n",
    "\n",
    "#get current year\n",
    "from datetime import date\n",
    "now = date.today().year\n",
    "\n",
    "\n",
    "def age_df(datacopy):\n",
    "\n",
    "    if (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] > 0) :\n",
    "        return (now - datacopy['DatoSaneri'])\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri']== 0):\n",
    "        return (now - datacopy['anlag_aar'])\n",
    "    elif (datacopy['TVObsKode'] == 0) and (datacopy['DatoSaneri'] > 0):\n",
    "        return (now - datacopy['DatoSaneri'])\n",
    "    elif (datacopy['TVObsKode']== 0) and (datacopy['DatoSaneri']== 0):\n",
    "        return (now - datacopy['anlag_aar'])\n",
    "\n",
    "datacopy['Age'] = datacopy.apply(age_df, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column 'PipeStatus'\n",
    "# 1 as broken and 0 as not broken\n",
    "\n",
    "def broken_df(datacopy):\n",
    "\n",
    "    if (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] < (datacopy['DatoOpdate'])) and (datacopy['DatoSaneri'] != 0):\n",
    "        return 1\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] >= (datacopy['DatoOpdate'])) and (datacopy['DatoSaneri'] != 0):\n",
    "        return 0\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri']== 0):\n",
    "        return 1\n",
    "    elif (datacopy['TVObsKode'] == 0) and (datacopy['DatoSaneri'] > 0):\n",
    "        return 0\n",
    "    elif (datacopy['TVObsKode']== 0) and (datacopy['DatoSaneri']== 0):\n",
    "        return 0\n",
    "\n",
    "datacopy['PipeStatus'] = datacopy.apply(broken_df, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacopy = datacopy.sample(n=22) \n",
    "# datacopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fs= np.where(np.isnan(datacopy))\n",
    "# data_fs\n",
    "# row = datacopy.iloc[369] #index=1 => second row\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 6231\n",
      "Number of rows after removing NaNs: 6231\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(datacopy.shape[0]))\n",
    "datacopy = datacopy.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(datacopy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fra_kote</th>\n",
       "      <th>til_kote</th>\n",
       "      <th>Laengde</th>\n",
       "      <th>Fald</th>\n",
       "      <th>DiameterIn</th>\n",
       "      <th>MaterialeK</th>\n",
       "      <th>anlag_aar</th>\n",
       "      <th>TransportK</th>\n",
       "      <th>Funktionsk</th>\n",
       "      <th>TVObsKode</th>\n",
       "      <th>DatoSaneri</th>\n",
       "      <th>Age</th>\n",
       "      <th>PipeStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>34.72</td>\n",
       "      <td>33.48</td>\n",
       "      <td>64.88</td>\n",
       "      <td>19.112207</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>39.46</td>\n",
       "      <td>39.16</td>\n",
       "      <td>91.75</td>\n",
       "      <td>3.269755</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>39.71</td>\n",
       "      <td>39.48</td>\n",
       "      <td>87.69</td>\n",
       "      <td>2.622876</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>40.55</td>\n",
       "      <td>40.08</td>\n",
       "      <td>52.11</td>\n",
       "      <td>9.019382</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>40.38</td>\n",
       "      <td>40.55</td>\n",
       "      <td>68.39</td>\n",
       "      <td>-2.485744</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>3.92</td>\n",
       "      <td>3.95</td>\n",
       "      <td>79.50</td>\n",
       "      <td>-0.377358</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17292</th>\n",
       "      <td>45.17</td>\n",
       "      <td>45.01</td>\n",
       "      <td>45.10</td>\n",
       "      <td>3.547672</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1954.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>40.86</td>\n",
       "      <td>40.74</td>\n",
       "      <td>38.80</td>\n",
       "      <td>3.092784</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16430</th>\n",
       "      <td>19.73</td>\n",
       "      <td>19.47</td>\n",
       "      <td>9.22</td>\n",
       "      <td>28.199566</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>52.16</td>\n",
       "      <td>51.50</td>\n",
       "      <td>78.62</td>\n",
       "      <td>8.394810</td>\n",
       "      <td>160.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6230 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fra_kote  til_kote  Laengde       Fald  DiameterIn  MaterialeK  \\\n",
       "36        34.72     33.48    64.88  19.112207       300.0         1.0   \n",
       "42        39.46     39.16    91.75   3.269755       400.0         1.0   \n",
       "43        39.71     39.48    87.69   2.622876       300.0         1.0   \n",
       "64        40.55     40.08    52.11   9.019382       250.0         1.0   \n",
       "65        40.38     40.55    68.39  -2.485744       250.0         1.0   \n",
       "...         ...       ...      ...        ...         ...         ...   \n",
       "3423       3.92      3.95    79.50  -0.377358       150.0         4.0   \n",
       "17292     45.17     45.01    45.10   3.547672       300.0         1.0   \n",
       "1746      40.86     40.74    38.80   3.092784       700.0         1.0   \n",
       "16430     19.73     19.47     9.22  28.199566       200.0         1.0   \n",
       "237       52.16     51.50    78.62   8.394810       160.0         4.0   \n",
       "\n",
       "       anlag_aar  TransportK  Funktionsk  TVObsKode  DatoSaneri   Age  \\\n",
       "36        1939.0           1           0        0.0      1997.0  24.0   \n",
       "42        1939.0           1           0        1.0         0.0  82.0   \n",
       "43        1939.0           1           0        1.0         0.0  82.0   \n",
       "64        1945.0           1           0        1.0         0.0  76.0   \n",
       "65        1945.0           1           0        1.0         0.0  76.0   \n",
       "...          ...         ...         ...        ...         ...   ...   \n",
       "3423      2000.0           1           2        0.0         0.0  21.0   \n",
       "17292     1954.0           1           0        0.0         0.0  67.0   \n",
       "1746      1962.0           1           0        0.0         0.0  59.0   \n",
       "16430     1964.0           1           0        0.0         0.0  57.0   \n",
       "237       1975.0           1           0        0.0         0.0  46.0   \n",
       "\n",
       "       PipeStatus  \n",
       "36              0  \n",
       "42              1  \n",
       "43              1  \n",
       "64              1  \n",
       "65              1  \n",
       "...           ...  \n",
       "3423            0  \n",
       "17292           0  \n",
       "1746            0  \n",
       "16430           0  \n",
       "237             0  \n",
       "\n",
       "[6230 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop columns not needed after adding new features\n",
    "\n",
    "columns_to_be_removed = ['DatoOprett', 'DatoOpdate']\n",
    "datacopy=datacopy.drop(columns_to_be_removed,axis='columns')\n",
    "datacopy[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features set and target\n",
    "\n",
    "columns_to_be_removed = ['PipeStatus']\n",
    "data_features= datacopy.drop(columns_to_be_removed,axis='columns')\n",
    "columns_to_be_removed = ['fra_kote','til_kote', 'Laengde','Fald','DiameterIn','MaterialeK','anlag_aar','TransportK','Funktionsk','TVObsKode','DatoSaneri','Age']\n",
    "data_target=datacopy.drop(columns_to_be_removed,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 6231\n",
      "Number of rows after removing NaNs: 6231\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(data.shape[0]))\n",
    "data = data.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1.0\n",
      "Best penalty: l2\n",
      "Accuracy on training set: 0.997\n",
      "Accuracy on test set: 0.991\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       0.99      0.99      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       0.99      0.99      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9924038589419623"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create logreg Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Create x and y variables.\n",
    "x = data_features\n",
    "y = data_target\n",
    "\n",
    "#Split data into training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3504\n",
      "Size of validation set:1169\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "print(\"Size of training set:{}\".format(X_train.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.9974337040205303\n",
      "Score on training/validation set: 0.9993580141236893\n",
      "Score on test set: 0.9961489088575096\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       1.00      0.97      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       1.00      0.99      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863013698630136"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameters with cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3504\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "print(\"Size of training set:{}\".format(X_train.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.9982878539774873\n",
      "Score on training/validation set: 0.9989300235394821\n",
      "Score on test set: 0.9955070603337612\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1339\n",
      "           1       1.00      0.97      0.98       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       1.00      0.98      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9840182648401826"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.1\n",
      "Best penalty: l1\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1337\n",
      "           1       0.93      1.00      0.96       221\n",
      "\n",
      "    accuracy                           0.99      1558\n",
      "   macro avg       0.96      0.99      0.98      1558\n",
      "weighted avg       0.99      0.99      0.99      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9936424831712789"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create forest Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.1\n",
      "Best penalty: l1\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      1337\n",
      "           1       0.93      1.00      0.96       221\n",
      "\n",
      "    accuracy                           0.99      1558\n",
      "   macro avg       0.96      0.99      0.98      1558\n",
      "weighted avg       0.99      0.99      0.99      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9936424831712789"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create forest Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3504\n",
      "Size of validation set:1169\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_train_scaled.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 1.0\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.9991445680068435\n",
      "Score on training/validation set: 0.9967900706184464\n",
      "Score on test set: 0.9961489088575096\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val_scaled, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       0.97      1.00      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       0.99      1.00      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9977595220313668"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3504\n",
      "Size of validation set:1169\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_train_scaled.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.9991445680068435\n",
      "Score on training/validation set: 0.9991440188315857\n",
      "Score on test set: 0.9961489088575096\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val_scaled, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       1.00      0.97      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       1.00      0.99      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863013698630136"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with cross validation split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:4673\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_trainval_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.9987158904831155\n",
      "Score on training/validation set: 0.9991440188315857\n",
      "Score on test set: 0.9961489088575096\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval_scaled, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       1.00      0.97      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       1.00      0.99      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863013698630136"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with cross validation split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:4673\n",
      "Size of test set:1558\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_trainval_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.9987161195021127\n",
      "Score on training/validation set: 0.9991440188315857\n",
      "Score on test set: 0.9961489088575096\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval_scaled, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1339\n",
      "           1       1.00      0.97      0.99       219\n",
      "\n",
      "    accuracy                           1.00      1558\n",
      "   macro avg       1.00      0.99      0.99      1558\n",
      "weighted avg       1.00      1.00      1.00      1558\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9863013698630136"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
