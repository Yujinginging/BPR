{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (confusion_matrix,precision_score,recall_score,f1_score,\n",
    "    roc_curve,roc_auc_score,precision_recall_curve,accuracy_score,classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Energi_Viborg_Dandas_data.csv\")\n",
    "\n",
    "#drop columns not needed after asking the company about the meaning of these features\n",
    "\n",
    "columns_to_be_removed = ['ID', 'mslink', 'XKoordinat','YKoordinat','LedningID','Dobbeltled','EjerKompon','SystemKode','KategoriAf','DatoUdf']\n",
    "data=data.drop(columns_to_be_removed,axis='columns')\n",
    "\n",
    "# in the column DatoSaneri is the date of repairing and if there is no date it means it is not repaired\n",
    "\n",
    "data['DatoSaneri'].fillna(0, inplace=True)\n",
    "\n",
    "# take only the pipes that are broken(by TV insection) now and the repaired ones\n",
    "\n",
    "data_with_TVObsAndSaneri = data[data['TVObsKode'].isin([1]) | data['DatoSaneri'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get around 2077 rows with not broken pipes\n",
    "\n",
    "data_not_broken = data[~data['TVObsKode'].isin([0]) | data['DatoSaneri'] == 0]\n",
    "data_not_broken = data_not_broken.sample(n=2077) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fra_kote</th>\n",
       "      <th>til_kote</th>\n",
       "      <th>Laengde</th>\n",
       "      <th>Fald</th>\n",
       "      <th>DiameterIn</th>\n",
       "      <th>MaterialeK</th>\n",
       "      <th>anlag_aar</th>\n",
       "      <th>TransportK</th>\n",
       "      <th>Funktionsk</th>\n",
       "      <th>TVObsKode</th>\n",
       "      <th>DatoOprett</th>\n",
       "      <th>DatoOpdate</th>\n",
       "      <th>DatoSaneri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>34.72</td>\n",
       "      <td>33.48</td>\n",
       "      <td>64.88</td>\n",
       "      <td>19.112207</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>1997.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>39.46</td>\n",
       "      <td>39.16</td>\n",
       "      <td>91.75</td>\n",
       "      <td>3.269755</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>39.71</td>\n",
       "      <td>39.48</td>\n",
       "      <td>87.69</td>\n",
       "      <td>2.622876</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>40.55</td>\n",
       "      <td>40.08</td>\n",
       "      <td>52.11</td>\n",
       "      <td>9.019382</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>40.38</td>\n",
       "      <td>40.55</td>\n",
       "      <td>68.39</td>\n",
       "      <td>-2.485744</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10374</th>\n",
       "      <td>49.85</td>\n",
       "      <td>49.14</td>\n",
       "      <td>60.54</td>\n",
       "      <td>11.727783</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14100</th>\n",
       "      <td>43.68</td>\n",
       "      <td>43.39</td>\n",
       "      <td>15.27</td>\n",
       "      <td>18.991487</td>\n",
       "      <td>240.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>17.61</td>\n",
       "      <td>16.66</td>\n",
       "      <td>99.58</td>\n",
       "      <td>9.540068</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>32.31</td>\n",
       "      <td>31.89</td>\n",
       "      <td>38.97</td>\n",
       "      <td>10.777521</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17556</th>\n",
       "      <td>6.96</td>\n",
       "      <td>6.76</td>\n",
       "      <td>9.34</td>\n",
       "      <td>21.413276</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4154 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fra_kote  til_kote  Laengde       Fald  DiameterIn  MaterialeK  \\\n",
       "36        34.72     33.48    64.88  19.112207       300.0         1.0   \n",
       "42        39.46     39.16    91.75   3.269755       400.0         1.0   \n",
       "43        39.71     39.48    87.69   2.622876       300.0         1.0   \n",
       "64        40.55     40.08    52.11   9.019382       250.0         1.0   \n",
       "65        40.38     40.55    68.39  -2.485744       250.0         1.0   \n",
       "...         ...       ...      ...        ...         ...         ...   \n",
       "10374     49.85     49.14    60.54  11.727783       200.0         4.0   \n",
       "14100     43.68     43.39    15.27  18.991487       240.0         4.0   \n",
       "12106     17.61     16.66    99.58   9.540068       200.0         1.0   \n",
       "7001      32.31     31.89    38.97  10.777521       250.0         4.0   \n",
       "17556      6.96      6.76     9.34  21.413276       200.0         1.0   \n",
       "\n",
       "       anlag_aar  TransportK  Funktionsk  TVObsKode  DatoOprett  DatoOpdate  \\\n",
       "36        1939.0           1           0        0.0        2010        2014   \n",
       "42        1939.0           1           0        1.0        2010        2014   \n",
       "43        1939.0           1           0        1.0        2010        2014   \n",
       "64        1945.0           1           0        1.0        2010        2014   \n",
       "65        1945.0           1           0        1.0        2010        2014   \n",
       "...          ...         ...         ...        ...         ...         ...   \n",
       "10374     2008.0           1           0        0.0        2010        2014   \n",
       "14100     1964.0           1           0        0.0        2010        2014   \n",
       "12106     1976.0           1           0        0.0        2010        2014   \n",
       "7001      2000.0           1           0        0.0        2010        2014   \n",
       "17556     1980.0           1           0        0.0        2012        2014   \n",
       "\n",
       "       DatoSaneri  \n",
       "36         1997.0  \n",
       "42            0.0  \n",
       "43            0.0  \n",
       "64            0.0  \n",
       "65            0.0  \n",
       "...           ...  \n",
       "10374         0.0  \n",
       "14100         0.0  \n",
       "12106         0.0  \n",
       "7001          0.0  \n",
       "17556         0.0  \n",
       "\n",
       "[4154 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [data_with_TVObsAndSaneri, data_not_broken]\n",
    "  \n",
    "data_final = pd.concat(frames)\n",
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 4154\n",
      "Number of rows after removing NaNs: 4154\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(data.shape[0]))\n",
    "data = data.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data copied\n",
    "datacopy = data\n",
    "\n",
    "\n",
    "# add  age column\n",
    "\n",
    "#get current year\n",
    "from datetime import date\n",
    "now = date.today().year\n",
    "\n",
    "\n",
    "def age_df(datacopy):\n",
    "\n",
    "    if (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] > 0) :\n",
    "        return (now - datacopy['DatoSaneri'])\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri']== 0):\n",
    "        return (now - datacopy['anlag_aar'])\n",
    "    elif (datacopy['TVObsKode'] == 0) and (datacopy['DatoSaneri'] > 0):\n",
    "        return (now - datacopy['DatoSaneri'])\n",
    "    elif (datacopy['TVObsKode']== 0) and (datacopy['DatoSaneri']== 0):\n",
    "        return (now - datacopy['anlag_aar'])\n",
    "\n",
    "datacopy['Age'] = datacopy.apply(age_df, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column 'PipeStatus'\n",
    "# 1 as broken and 0 as not broken\n",
    "\n",
    "def broken_df(datacopy):\n",
    "\n",
    "    if (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] < (datacopy['DatoOpdate'])) and (datacopy['DatoSaneri'] != 0):\n",
    "        return 1\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri'] >= (datacopy['DatoOpdate'])) and (datacopy['DatoSaneri'] != 0):\n",
    "        return 0\n",
    "    elif (datacopy['TVObsKode'] == 1) and (datacopy['DatoSaneri']== 0):\n",
    "        return 1\n",
    "    elif (datacopy['TVObsKode'] == 0) and (datacopy['DatoSaneri'] > 0):\n",
    "        return 0\n",
    "    elif (datacopy['TVObsKode']== 0) and (datacopy['DatoSaneri']== 0):\n",
    "        return 0\n",
    "\n",
    "datacopy['PipeStatus'] = datacopy.apply(broken_df, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datacopy = datacopy.sample(n=22) \n",
    "# datacopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_fs= np.where(np.isnan(datacopy))\n",
    "# data_fs\n",
    "# row = datacopy.iloc[369] #index=1 => second row\n",
    "# print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 4154\n",
      "Number of rows after removing NaNs: 4154\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(datacopy.shape[0]))\n",
    "datacopy = datacopy.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(datacopy.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fra_kote</th>\n",
       "      <th>til_kote</th>\n",
       "      <th>Laengde</th>\n",
       "      <th>Fald</th>\n",
       "      <th>DiameterIn</th>\n",
       "      <th>MaterialeK</th>\n",
       "      <th>anlag_aar</th>\n",
       "      <th>TransportK</th>\n",
       "      <th>Funktionsk</th>\n",
       "      <th>TVObsKode</th>\n",
       "      <th>DatoSaneri</th>\n",
       "      <th>Age</th>\n",
       "      <th>PipeStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>34.72</td>\n",
       "      <td>33.48</td>\n",
       "      <td>64.88</td>\n",
       "      <td>19.112207</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>39.46</td>\n",
       "      <td>39.16</td>\n",
       "      <td>91.75</td>\n",
       "      <td>3.269755</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>39.71</td>\n",
       "      <td>39.48</td>\n",
       "      <td>87.69</td>\n",
       "      <td>2.622876</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>40.55</td>\n",
       "      <td>40.08</td>\n",
       "      <td>52.11</td>\n",
       "      <td>9.019382</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>40.38</td>\n",
       "      <td>40.55</td>\n",
       "      <td>68.39</td>\n",
       "      <td>-2.485744</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1945.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13261</th>\n",
       "      <td>26.76</td>\n",
       "      <td>26.25</td>\n",
       "      <td>106.29</td>\n",
       "      <td>4.798194</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10374</th>\n",
       "      <td>49.85</td>\n",
       "      <td>49.14</td>\n",
       "      <td>60.54</td>\n",
       "      <td>11.727783</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14100</th>\n",
       "      <td>43.68</td>\n",
       "      <td>43.39</td>\n",
       "      <td>15.27</td>\n",
       "      <td>18.991487</td>\n",
       "      <td>240.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>17.61</td>\n",
       "      <td>16.66</td>\n",
       "      <td>99.58</td>\n",
       "      <td>9.540068</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>32.31</td>\n",
       "      <td>31.89</td>\n",
       "      <td>38.97</td>\n",
       "      <td>10.777521</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4153 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fra_kote  til_kote  Laengde       Fald  DiameterIn  MaterialeK  \\\n",
       "36        34.72     33.48    64.88  19.112207       300.0         1.0   \n",
       "42        39.46     39.16    91.75   3.269755       400.0         1.0   \n",
       "43        39.71     39.48    87.69   2.622876       300.0         1.0   \n",
       "64        40.55     40.08    52.11   9.019382       250.0         1.0   \n",
       "65        40.38     40.55    68.39  -2.485744       250.0         1.0   \n",
       "...         ...       ...      ...        ...         ...         ...   \n",
       "13261     26.76     26.25   106.29   4.798194       250.0         1.0   \n",
       "10374     49.85     49.14    60.54  11.727783       200.0         4.0   \n",
       "14100     43.68     43.39    15.27  18.991487       240.0         4.0   \n",
       "12106     17.61     16.66    99.58   9.540068       200.0         1.0   \n",
       "7001      32.31     31.89    38.97  10.777521       250.0         4.0   \n",
       "\n",
       "       anlag_aar  TransportK  Funktionsk  TVObsKode  DatoSaneri   Age  \\\n",
       "36        1939.0           1           0        0.0      1997.0  24.0   \n",
       "42        1939.0           1           0        1.0         0.0  82.0   \n",
       "43        1939.0           1           0        1.0         0.0  82.0   \n",
       "64        1945.0           1           0        1.0         0.0  76.0   \n",
       "65        1945.0           1           0        1.0         0.0  76.0   \n",
       "...          ...         ...         ...        ...         ...   ...   \n",
       "13261     1943.0           1           0        0.0         0.0  78.0   \n",
       "10374     2008.0           1           0        0.0         0.0  13.0   \n",
       "14100     1964.0           1           0        0.0         0.0  57.0   \n",
       "12106     1976.0           1           0        0.0         0.0  45.0   \n",
       "7001      2000.0           1           0        0.0         0.0  21.0   \n",
       "\n",
       "       PipeStatus  \n",
       "36              0  \n",
       "42              1  \n",
       "43              1  \n",
       "64              1  \n",
       "65              1  \n",
       "...           ...  \n",
       "13261           0  \n",
       "10374           0  \n",
       "14100           0  \n",
       "12106           0  \n",
       "7001            0  \n",
       "\n",
       "[4153 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop columns not needed after adding new features\n",
    "\n",
    "columns_to_be_removed = ['DatoOprett', 'DatoOpdate']\n",
    "datacopy=datacopy.drop(columns_to_be_removed,axis='columns')\n",
    "datacopy[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features set and target\n",
    "\n",
    "columns_to_be_removed = ['PipeStatus']\n",
    "data_features= datacopy.drop(columns_to_be_removed,axis='columns')\n",
    "columns_to_be_removed = ['fra_kote','til_kote', 'Laengde','Fald','DiameterIn','MaterialeK','anlag_aar','TransportK','Funktionsk','TVObsKode','DatoSaneri','Age']\n",
    "data_target=datacopy.drop(columns_to_be_removed,axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NaNs: 4154\n",
      "Number of rows after removing NaNs: 4154\n"
     ]
    }
   ],
   "source": [
    "# data_fs= np.where(np.isnan(data_features))\n",
    "# data_fs\n",
    "print(\"Number of rows before removing NaNs: {}\".format(data.shape[0]))\n",
    "data = data.dropna()\n",
    "print(\"Number of rows after removing NaNs: {}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 10\n",
      "Best penalty: l2\n",
      "Accuracy on training set: 0.995\n",
      "Accuracy on test set: 0.986\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       835\n",
      "           1       0.96      1.00      0.98       204\n",
      "\n",
      "    accuracy                           0.99      1039\n",
      "   macro avg       0.98      0.99      0.99      1039\n",
      "weighted avg       0.99      0.99      0.99      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9946107784431139"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create forest Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Create x and y variables.\n",
    "x = data_features\n",
    "y = data_target\n",
    "\n",
    "#Split data into training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:2336\n",
      "Size of validation set:779\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "print(\"Size of training set:{}\".format(X_train.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.993581514762516\n",
      "Score on training/validation set: 0.9967897271268058\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      0.99      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9950980392156863"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameters with cross validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:2336\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "print(\"Size of training set:{}\".format(X_train.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.9961476725521671\n",
      "Score on training/validation set: 0.9974317817014446\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      0.99      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9950980392156863"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.1\n",
      "Best penalty: l1\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       818\n",
      "           1       0.94      1.00      0.97       221\n",
      "\n",
      "    accuracy                           0.99      1039\n",
      "   macro avg       0.97      0.99      0.98      1039\n",
      "weighted avg       0.99      0.99      0.99      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.991442542787286"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create forest Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with train-test split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_features, data_target, stratify=data_target, random_state=42)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.1\n",
      "Best penalty: l1\n",
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 1.000\n"
     ]
    }
   ],
   "source": [
    "#List Hyperparameters that we want to tune by cross validation\n",
    "\n",
    "C = [100, 10, 1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(C = C , penalty= penalty)\n",
    "\n",
    "#Create new logistic object\n",
    "logreg = LogisticRegression( solver = 'liblinear')\n",
    "\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(logreg, hyperparameters, cv=5,return_train_score=True, scoring=\"recall\")\n",
    "\n",
    "#Fit the model\n",
    "best_model = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Print The value of best Hyperparameters\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "    \n",
    "print(\"Accuracy on training set: {:.3f}\".format(best_model.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       818\n",
      "           1       0.94      1.00      0.97       221\n",
      "\n",
      "    accuracy                           0.99      1039\n",
      "   macro avg       0.97      0.99      0.98      1039\n",
      "weighted avg       0.99      0.99      0.99      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.991442542787286"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create forest Object.\n",
    "a = best_model.best_estimator_.get_params()['C']\n",
    "b = best_model.best_estimator_.get_params()['penalty']\n",
    "\n",
    "logreg = LogisticRegression(C = a, penalty= b, solver = 'liblinear')\n",
    "\n",
    "#Training the model.\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#Predict test data set.\n",
    "y_pred =logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:2336\n",
      "Size of validation set:779\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_train_scaled.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.993581514762516\n",
      "Score on training/validation set: 0.9983948635634029\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val_scaled, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      1.00      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969502172126334"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with validation set split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:2336\n",
      "Size of validation set:779\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training, test and validation\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=43)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_train_scaled.shape[0]))\n",
    "print(\"Size of validation set:{}\".format(X_val_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best score on validation set: 0.9974326059050064\n",
      "Score on training/validation set: 0.9990369181380417\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        \n",
    "        # Learn the model\n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        logreg.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        score = logreg.score(X_val_scaled, y_val)\n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "                        \n",
    "\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C =best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best score on validation set: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      1.00      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969502172126334"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with cross validation split and Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3115\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "\n",
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_trainval_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.9967897271268058\n",
      "Score on training/validation set: 0.9983948635634029\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval_scaled, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      1.00      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969502172126334"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with cross validation split and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:3115\n",
      "Size of test set:1039\n"
     ]
    }
   ],
   "source": [
    "# Divide the data into training and test , no validation data\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(data_features, data_target, random_state=42)\n",
    "\n",
    "# preprocessing using zero mean and unit variance scaling\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_trainval_scaled = scaler.transform( X_trainval)\n",
    "\n",
    "\n",
    "print(\"Size of training set:{}\".format(X_trainval_scaled.shape[0]))\n",
    "print(\"Size of test set:{}\".format(X_test_scaled.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best best_C found: 100\n",
      "Best best_penalty found: l1\n",
      "Best average score: 0.997752808988764\n",
      "Score on training/validation set: 0.9990369181380417\n",
      "Score on test set: 0.9980750721847931\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "\n",
    "for C in [100, 10, 1.0, 0.1, 0.01, 0.001]:\n",
    "    for penalty in ['l1', 'l2']:\n",
    "        # Learn the model \n",
    "        logreg = LogisticRegression(penalty=penalty, C=C, solver = 'liblinear')\n",
    "        \n",
    "        # Perform cross validation\n",
    "        scores = cross_val_score(logreg, X_trainval_scaled, y_trainval, cv=5)\n",
    "        \n",
    "        # Compute the mean score\n",
    "        score = scores.mean()\n",
    "        \n",
    "        \n",
    "        # If improvement, store score and parameter\n",
    "        if score>best_score:\n",
    "            best_score = score\n",
    "            best_C = C\n",
    "            best_penalty= penalty\n",
    "\n",
    "# Build a model on the combine training and valiation data\n",
    "logreg = LogisticRegression(penalty= best_penalty, C=best_C, solver = 'liblinear')\n",
    "logreg.fit(X_trainval_scaled, y_trainval)\n",
    "\n",
    "print(\"Best best_C found: {}\".format(best_C))\n",
    "print(\"Best best_penalty found: {}\".format(best_penalty))\n",
    "print(\"Best average score: {}\".format(best_score))\n",
    "print(\"Score on training/validation set: {}\".format(logreg.score(X_trainval_scaled, y_trainval)))\n",
    "print(\"Score on test set: {}\".format(logreg.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       835\n",
      "           1       1.00      1.00      1.00       204\n",
      "\n",
      "    accuracy                           1.00      1039\n",
      "   macro avg       1.00      1.00      1.00      1039\n",
      "weighted avg       1.00      1.00      1.00      1039\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9969502172126334"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict test data set.\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "#Checking performance our model with classification report.\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#Checking performance our model with ROC Score.\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
